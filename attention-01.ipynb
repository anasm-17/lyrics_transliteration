{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code in this notebook is borrowed from \"TensorFlow Core\" at: https://www.tensorflow.org/tutorials/text/nmt_with_attention  \n",
    "### There may be some local change for study purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    fname = 'spa-eng.zip', \n",
    "    origin = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', \n",
    "    extract = True)\n",
    "filepath = os.path.dirname(path_to_zip) + '/spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n",
    "    w = re.sub(r'[\"\"]', \" \", w)\n",
    "    \n",
    "    # add space between chinese characters without affecting english letters\n",
    "    w = re.sub(r'(?<=[^a-z\\W\\d_])(?=[^a-z\\W\\d_])', ' ', w)\n",
    "    \n",
    "    # replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "#     w = re.sub(r'[^a-zA-Z?.,!¿]', ' ', w)\n",
    "    \n",
    "    w.rstrip().strip()\n",
    "    \n",
    "    # add a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this function when we have a dataset; use the fuction below for now\n",
    "```python\n",
    "# remove the accent & clean sentences & return word pairs [eng, spn]\n",
    "def create_dataset(path, num_exmaples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_exmaples]]\n",
    "    return zip(*word_pairs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(url):\n",
    "    html_content = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_content)\n",
    "    lines = soup.find_all(class_=\"ltf\")\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.get_text().split('\\n')] for l in lines]\n",
    "#     return zip(*word_pairs)\n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> 把 我 的 爱 情 还 给 我 <end>\n",
      "<start> ba wo de aiqing hai gei wo <end>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://lyricstranslate.com/en/nǐ-zěnme-shuō-你怎么说-nǐ-zěnme-shuō.html\"\n",
    "aa, bb = create_dataset(url)\n",
    "print(aa[-1])\n",
    "print(bb[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    \n",
    "    # get word to index dictionary for sequences\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    \n",
    "    # pad converted sequences\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    inp_lang, targ_lang = create_dataset(path)\n",
    "    \n",
    "    inp_tensor, inp_token = tokenize(inp_lang)\n",
    "    targ_tensor, targ_token = tokenize(targ_lang)\n",
    "    return inp_tensor, targ_tensor, inp_token, targ_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, input_token, target_token = load_dataset(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_input = max_length(input_tensor)\n",
    "max_length_target = max_length(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 4 15 4\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_valid, target_tensor_train, target_tensor_valid = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "print(len(input_tensor_train), len(input_tensor_valid), len(target_tensor_train), len(target_tensor_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(token, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(f\"{t} -----> {token.index_word[t]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input index ------> input language\n",
      "==================================\n",
      "1 -----> <start>\n",
      "42 -----> 连\n",
      "43 -----> 名\n",
      "44 -----> 字\n",
      "4 -----> 你\n",
      "14 -----> 都\n",
      "6 -----> 说\n",
      "45 -----> 错\n",
      "2 -----> <end>\n",
      "\n",
      "1 -----> <start>\n",
      "33 -----> lian\n",
      "34 -----> mingzi\n",
      "4 -----> ni\n",
      "8 -----> dou\n",
      "9 -----> shuo\n",
      "35 -----> cuo\n",
      "2 -----> <end>\n"
     ]
    }
   ],
   "source": [
    "print('Input index ------> input language')\n",
    "print('==================================')\n",
    "convert(input_token, input_tensor_train[0])\n",
    "print()\n",
    "convert(target_token, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 4\n",
    "steps_per_epoch = BUFFER_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_input_size = len(input_token.word_index) + 1\n",
    "vocab_target_size = len(target_token.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 18]), TensorShape([4, 16]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
